{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2-Investigation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avivajpeyi/investigating-gpt2/blob/master/GPT2_Investigation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxtrR-4xQLj7",
        "colab_type": "text"
      },
      "source": [
        "# Investigating OpenAI's GPT-2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAZP0RTAFqh3",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we can try out a small version (345M parameter vs their full 1.5B parameter model) of Open AI's GPT-2 model, described from the paper [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). OpenAI has also published a more human freindly [blog post](https://openai.com/blog/better-language-models/) about the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08tmn3F5RDWO",
        "colab_type": "text"
      },
      "source": [
        "## Notes about model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmnLLM0NUJtl",
        "colab_type": "text"
      },
      "source": [
        "### Background\n",
        "The GPT-2 algorithm was trained on the task of *language modeling*--- which tests a program's ability to predict the next word in a given sentence--by ingesting a shit ton of text data (40GB of articles, blogs, and websites,). By using this data it achieved \n",
        "\n",
        "*   \"*zero-shot learning*\": state-of-the-art scores on a number of unseen language tests\n",
        "*   [Unintentional (albeit not-so-great) Language translation](https://i1.wp.com/slatestarcodex.com/blog_images/english-french.png?zoom=2&w=700)\n",
        "*   *TLDR summarization*\n",
        "*   *Text completion*\n",
        "*  *Reading comprehension*\n",
        "*  [Essay Writing](https://pbs.twimg.com/media/DzYpsJOU0AA1PO9.png:large)\n",
        "*   more...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1daWxZIULF7",
        "colab_type": "text"
      },
      "source": [
        "### Why this is cool\n",
        "*   Another step to AGI\n",
        "*   Improving qs+a\n",
        "*   Recovering historical data (interpolation through text)\n",
        "*   Help explain difficult to understand texts to non native eglish speakers/novices\n",
        "*   Potentially use it as a tool to weed out \"fake news\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l92gCG9VUG6n",
        "colab_type": "text"
      },
      "source": [
        "### Why its scary: FAKE NEWS\n",
        "\n",
        "Open AI hasnt released the dataset, training code, or the full GPT-2 model weights. This is due to the concerns about large language models being used to generate deceptive, biased, or abusive language at scale. Some examples of the applications of these models for malicious purposes are:\n",
        "* Generate misleading news articles\n",
        "* Impersonate others online\n",
        "* Automate the production of abusive or faked content to post on social media\n",
        "* Automate the production of spam/phishing content\n",
        "\n",
        "As one can imagine, this combined with recent advances in generation of synthetic imagery, audio, and video implies that it's never been easier to create fake content and spread disinformation at scale (check out [this paper](http://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf) on making an AI that synthesizes photorealistic, lip-synced [video](https://www.youtube.com/watch?v=9Yq67CjDqvw) of Obama). The public at large will need to become more skeptical of the content they consume online. \n",
        "\n",
        "Also scary beceause this is yet another step to AGI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO9_2RgBVn_i",
        "colab_type": "text"
      },
      "source": [
        "### My opinions\n",
        "I think GPT-2 is a brute-force statistical pattern matcher which chews up the internet and gives you back a slightly confusing dump of it when asked... It seems like a plagarism model, that changes so much based on the large dataset that, there is no more of the original context... so its no longer plagarism? \n",
        "\n",
        "From what I understand, there is not really a new algorithmic contribution here. I think they are scaling previous research. But I think seeing exactly how strong these scaled up models an awesome contribution and challenge. It’s easy to say in retrospect “of course more data and compute gives you better models”.\n",
        "\n",
        "#### Skeptical\n",
        "I am a little skeptical on OpenAi's results -- they havent released the full model -- and they probably shouldnt, but have they just cherry-picked their results? Need to look more into this.\n",
        "\n",
        "Maybe trying out their 345M Parameter model will make me a believer! \n",
        "\n",
        "#### Is it really closer to a true AGI?\n",
        "Sure, this could do cool stuff with infinite training data and limitless computing resources, but thats true of alot of ML projects. Scaling the data needed to learn is tough problem. \n",
        "\n",
        "A true AGI will have to be much better at learning from limited datasets with limited computational resources. It will have to investigate the physical world with the same skill that GPT-2 investigates text....\n",
        "\n",
        "#### Wake up non-believers! \n",
        "AIs already pick up abilities that we dont expect them to learn, eg English-to-French translation without any French texts in their training corpus. GPT-2 is a good example of how AIs only learn what you program them to learn, and that they can do more than one specific task.\n",
        "\n",
        "#### Should it be released? Tough qs! \n",
        "The resources needed to train the full model are beyond the average person and small companies which could use this for potentially very interesting non-malicious applications. However large organizations and state actors that are most likely to use this for malicious purposes can and typically do already have easy access to the resources needed to replicate the full model.\n",
        "\n",
        "Therefore by not releasing the full model \"Open\"Ai is in a way ensuring that this sort of AI tech remains in the hands of powerful organizations and state actors that are most likely to misuse it while at the same time unintentionally tricking the general public to think this tech is not \"really\" available yet...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pokp-KpsQy1P",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2qtkGuIQCoq",
        "colab_type": "code",
        "outputId": "da18d3e6-8b52-41c7-e889-3677b62e6003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# testing GPU connection\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found; Go to Runtime > Change Runtime Type > Harware Accelerator > GPU')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX80K4gFOpTC",
        "colab_type": "code",
        "outputId": "bcb90ee9-3b43-44ae-a5f5-0933d5ef1ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Clone repo \n",
        "! git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 174, done.\u001b[K\n",
            "Receiving objects:   0% (1/174)   \rReceiving objects:   1% (2/174)   \rReceiving objects:   2% (4/174)   \rReceiving objects:   3% (6/174)   \rReceiving objects:   4% (7/174)   \rReceiving objects:   5% (9/174)   \rReceiving objects:   6% (11/174)   \rReceiving objects:   7% (13/174)   \rReceiving objects:   8% (14/174)   \rReceiving objects:   9% (16/174)   \rReceiving objects:  10% (18/174)   \rReceiving objects:  11% (20/174)   \rReceiving objects:  12% (21/174)   \rReceiving objects:  13% (23/174)   \rReceiving objects:  14% (25/174)   \rReceiving objects:  15% (27/174)   \rReceiving objects:  16% (28/174)   \rReceiving objects:  17% (30/174)   \rReceiving objects:  18% (32/174)   \rReceiving objects:  19% (34/174)   \rReceiving objects:  20% (35/174)   \rReceiving objects:  21% (37/174)   \rReceiving objects:  22% (39/174)   \rReceiving objects:  23% (41/174)   \rReceiving objects:  24% (42/174)   \rReceiving objects:  25% (44/174)   \rReceiving objects:  26% (46/174)   \rReceiving objects:  27% (47/174)   \rReceiving objects:  28% (49/174)   \rReceiving objects:  29% (51/174)   \rReceiving objects:  30% (53/174)   \rReceiving objects:  31% (54/174)   \rReceiving objects:  32% (56/174)   \rReceiving objects:  33% (58/174)   \rremote: Total 174 (delta 0), reused 0 (delta 0), pack-reused 174\u001b[K\n",
            "Receiving objects:  34% (60/174)   \rReceiving objects:  35% (61/174)   \rReceiving objects:  36% (63/174)   \rReceiving objects:  37% (65/174)   \rReceiving objects:  38% (67/174)   \rReceiving objects:  39% (68/174)   \rReceiving objects:  40% (70/174)   \rReceiving objects:  41% (72/174)   \rReceiving objects:  42% (74/174)   \rReceiving objects:  43% (75/174)   \rReceiving objects:  44% (77/174)   \rReceiving objects:  45% (79/174)   \rReceiving objects:  46% (81/174)   \rReceiving objects:  47% (82/174)   \rReceiving objects:  48% (84/174)   \rReceiving objects:  49% (86/174)   \rReceiving objects:  50% (87/174)   \rReceiving objects:  51% (89/174)   \rReceiving objects:  52% (91/174)   \rReceiving objects:  53% (93/174)   \rReceiving objects:  54% (94/174)   \rReceiving objects:  55% (96/174)   \rReceiving objects:  56% (98/174)   \rReceiving objects:  57% (100/174)   \rReceiving objects:  58% (101/174)   \rReceiving objects:  59% (103/174)   \rReceiving objects:  60% (105/174)   \rReceiving objects:  61% (107/174)   \rReceiving objects:  62% (108/174)   \rReceiving objects:  63% (110/174)   \rReceiving objects:  64% (112/174)   \rReceiving objects:  65% (114/174)   \rReceiving objects:  66% (115/174)   \rReceiving objects:  67% (117/174)   \rReceiving objects:  68% (119/174)   \rReceiving objects:  69% (121/174)   \rReceiving objects:  70% (122/174)   \rReceiving objects:  71% (124/174)   \rReceiving objects:  72% (126/174)   \rReceiving objects:  73% (128/174)   \rReceiving objects:  74% (129/174)   \rReceiving objects:  75% (131/174)   \rReceiving objects:  76% (133/174)   \rReceiving objects:  77% (134/174)   \rReceiving objects:  78% (136/174)   \rReceiving objects:  79% (138/174)   \rReceiving objects:  80% (140/174)   \rReceiving objects:  81% (141/174)   \rReceiving objects:  82% (143/174)   \rReceiving objects:  83% (145/174)   \rReceiving objects:  84% (147/174)   \rReceiving objects:  85% (148/174)   \rReceiving objects:  86% (150/174)   \rReceiving objects:  87% (152/174)   \rReceiving objects:  88% (154/174)   \rReceiving objects:  89% (155/174)   \rReceiving objects:  90% (157/174)   \rReceiving objects:  91% (159/174)   \rReceiving objects:  92% (161/174)   \rReceiving objects:  93% (162/174)   \rReceiving objects:  94% (164/174)   \rReceiving objects:  95% (166/174)   \rReceiving objects:  96% (168/174)   \rReceiving objects:  97% (169/174)   \rReceiving objects:  98% (171/174)   \rReceiving objects:  99% (173/174)   \rReceiving objects: 100% (174/174)   \rReceiving objects: 100% (174/174), 4.35 MiB | 16.45 MiB/s, done.\n",
            "Resolving deltas:   0% (0/89)   \rResolving deltas:   1% (1/89)   \rResolving deltas:   2% (2/89)   \rResolving deltas:   7% (7/89)   \rResolving deltas:  10% (9/89)   \rResolving deltas:  11% (10/89)   \rResolving deltas:  14% (13/89)   \rResolving deltas:  19% (17/89)   \rResolving deltas:  22% (20/89)   \rResolving deltas:  26% (24/89)   \rResolving deltas:  32% (29/89)   \rResolving deltas:  33% (30/89)   \rResolving deltas:  37% (33/89)   \rResolving deltas:  38% (34/89)   \rResolving deltas:  40% (36/89)   \rResolving deltas:  55% (49/89)   \rResolving deltas:  60% (54/89)   \rResolving deltas:  74% (66/89)   \rResolving deltas:  75% (67/89)   \rResolving deltas:  78% (70/89)   \rResolving deltas:  85% (76/89)   \rResolving deltas:  93% (83/89)   \rResolving deltas: 100% (89/89)   \rResolving deltas: 100% (89/89), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYhj6EaWJRrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move to gpt-2 repo\n",
        "import os\n",
        "os.chdir('gpt-2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUEy5HipOtp0",
        "colab_type": "code",
        "outputId": "a7fdc2cd-7555-4df4-81ef-2722c3bd791a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# download model and install dependencies\n",
        "!python download_model.py 345M\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 746kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 51.0Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 585kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:22, 62.6Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.78Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 52.8Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 49.9Mit/s]                                                       \n",
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.1.3)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHDbz70yf9nj",
        "colab_type": "text"
      },
      "source": [
        "# Unconditional sample generation\n",
        "Generates text samples on the whim of the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuPcqKngSfn",
        "colab_type": "code",
        "outputId": "d0a109fe-2eae-4fcf-9c96-f8fec65b968a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3369
        }
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --model_name='345M' --nsamples=3 --top_k=10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-10 05:13:12.112505: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-10 05:13:12.112776: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2301700 executing computations on platform Host. Devices:\n",
            "2019-05-10 05:13:12.112813: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 05:13:12.252840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 05:13:12.253371: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2301440 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 05:13:12.253403: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 05:13:12.253774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.39GiB\n",
            "2019-05-10 05:13:12.253802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 05:13:12.782187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 05:13:12.782272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 05:13:12.782288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 05:13:12.782578: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-10 05:13:12.782645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13914 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-05-10 05:13:24.145678: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            "If you're a fan of the show, this is the perfect opportunity to get your tickets.\n",
            "\n",
            "If you're not a fan of the show, but just want to attend, here's the deal. You can reserve your spot for as long as you'd like. You don't have to do anything special. You don't have to bring anything special with you to your event. You don't have to show any of your ticket stubs to anyone, and you don't have to show anything to anyone else. You simply have to make an appointment, and we'll get you in the door.\n",
            "\n",
            "So, you can go to your local show with nothing more than a few minutes before the doors open. Just make sure you make an appointment before your time. And if you're in the area, you can reserve your spot online, so you'll know if you're going to be able to get in on time or not.\n",
            "\n",
            "The first 100 people who sign up will be entered into a draw, so you'll know who's going to get in. The more tickets you get, the greater your chance. So don't delay. Sign up now and get in on time!<|endoftext|>A few weeks ago, a friend of mine posted about a project I was involved on, and it caught my eye. I immediately thought it would be interesting to see if anyone would be interested in doing the following:\n",
            "As someone who works mostly in Java (or at least JavaScript), it's interesting to know what the language is like in comparison to other programming languages.\n",
            "As a JavaScript developer, it's interesting to know what the language is like in comparison to JavaScript. It can be interesting to see which programming languages are popular (e.g. Javascript is the second most popular language after C++ in the USA). As the author, I wanted to know if there was a language that people were familiar enough with that they would consider to be JavaScript that people would be familiar with.\n",
            "So here it is, the results of that survey.  I'll admit, there are some differences in the results of this study. The sample size was smaller, but the survey was done using the same survey tools for JavaScript in 2014 and 2015 as they have been for JavaScript in 2014 and 2015. I also didn't have a lot of control of the survey tools used. So there were some limitations to the sample size and to the methodology.\n",
            "I'll say it again, this is a survey, so there will still be people who are unfamiliar with JavaScript. There is no way to say how many of them have JavaScript knowledge but that doesn't mean they're not using it. The survey was done using Google Forms for JavaScript to be able to see how many people had knowledge of JavaScript.\n",
            "The survey questions were as follows: \"What are some of the most frequently used JavaScript constructs?\n",
            "What are some of the most commonly used JavaScript patterns? \" \"Do you have experience with using JavaScript in your professional job or at home?\" \" \"How long have you used JavaScript in your professional life?\" \"How long ago were you able to use JavaScript in your professional life?\"\n",
            "What I did was create a spreadsheet for each of the question. The questions were:\n",
            "1) How long have you used JavaScript in your professional life?\n",
            "2) How long ago were you able to use JavaScript in your professional life?\n",
            "3) Do you have experience using JavaScript in your professional job or at home?\n",
            "4) How long ago were you able to use JavaScript in your professional life?\n",
            "I've created some graphs below that show which programming languages I'm familiar with. You can get the spreadsheet here .\n",
            "I've created some graphs below to give you an idea of the numbers.\n",
            "I have a lot of fun with this. I've created two graphs that show how the language of choice is different from language of choice:\n",
            "1) JavaScript is more popular among women than is Java .\n",
            "1) I'm a JavaScript user but not a Java user.\n",
            "I can see this pattern. The language I use in my professional job is much more popular than the language I use in my professional life. I'm not sure why this should be. It seems like people use more languages than they actually use to get the job done so maybe that's a good sign? I also think there's a difference in the popularity in which languages are used. I guess the popularity of one particular language is a better indicator of which language is used than the popularity of a language of another.\n",
            "The question is, is JavaScript popular in the USA. I'm not sure what I'm going to use as an answer to this question. I've seen lots of people talk and argue over this in the comments of this blog post. I've even seen posts on Twitter that argue that I am an asshole because it's not as common as they say.\n",
            "So I've decided to take the next steps. If it were me I would take a look at a sample survey that is similar in the language\n",
            "======================================== SAMPLE 2 ========================================\n",
            "The United Nations has been accused of turning a blind eye to the growing number of refugees fleeing war zones.\n",
            "\n",
            "The UN's refugee agency, UNHCR, warned that refugees were being used by extremists in Iraq and Syria to spread terror.\n",
            "\n",
            "It has already seen an increase in the amount of people claiming refugee status in both countries.\n",
            "\n",
            "It is reported that some of the refugees from Syria have been radicalised through online media, with some using their connections in the UK and the US to get to the other side of the world.\n",
            "\n",
            "The UK has already announced plans to accept up to 2,000 Syrian refugees.\n",
            "\n",
            "It has been reported that up to 2,000 Syrian refugees are thought to have entered the country this year.\n",
            "\n",
            "UNICEF has also warned that up to 50,000 children in Syria and Iraq are at high risk of starvation and that some children have already died in camps.\n",
            "\n",
            "The organisation also warned that the influx could lead to a rise in the number of people seeking to join Islamic State in Iraq and Syria.\n",
            "\n",
            "\"As more Syrians seek to enter Iraq and to enter Syria, we will see a rise in the number of young, vulnerable children and girls who are seeking to join IS and the extremist groups they are fighting in order to join their families,\" said UNICEF.\n",
            "\n",
            "\"We call upon our partners, governments and non-governmental organizations to support them by providing food and shelter,\" it added.\n",
            "\n",
            "UN agencies in the Middle East have already begun to receive more refugees than previously.\n",
            "\n",
            "On Monday, the Syrian Arab Red Crescent, a humanitarian agency based in northern Lebanon, reported that it had received 1,600 new Syrians in a week's time.\n",
            "\n",
            "The group has been working for the Syrian refugees to reach their first official resettlement point in Turkey.\n",
            "\n",
            "It will also help more refugees who have yet to be resettled.\n",
            "\n",
            "In addition to the Syrian refugees, UNHCR is already receiving refugees from Yemen, Afghanistan, Somalia, and Bangladesh.\n",
            "\n",
            "The agency also received more than 1,000 people from Eritrea, Ethiopia, Somalia, Sudan, and Burundi on Monday.\n",
            "\n",
            "In the UK, it has also been reported that the UK will welcome up to 500 Syrian refugees, which will be resettled in a town near Oxfordshire.\n",
            "\n",
            "The decision is not expected to be finalised until next year.<|endoftext|>When it comes to food, we are all guilty of indulging in one or more indulgences: the potato chips that you make for lunch, the pretzel sticks that you snack on in public, etc. We are also guilty of indulging in another indulgence: the ice cream.\n",
            "\n",
            "But what's the best way to enjoy a delicious and decadent, yet healthy and tasty dessert?\n",
            "\n",
            "There are several reasons to consider:\n",
            "\n",
            "The ice cream is a delicious and healthy treat. The ice cream is a tasty dessert that is also healthy.\n",
            "\n",
            "What do all three of these have in common?\n",
            "\n",
            "All the above factors are true. However, what is the best way to enjoy the best ice cream of the day?\n",
            "\n",
            "In this article, we will focus on two factors:\n",
            "\n",
            "How much ice cream you have and\n",
            "\n",
            "When to use it.\n",
            "\n",
            "How much ice cream you have\n",
            "\n",
            "When to use it\n",
            "\n",
            "In order to make a delicious ice cream, you'll need to have at least 4 ounces (150 milliliters or 2.8 cups) of ice cream in your freezer, and ideally at least 12 ounces (400 milliliters or 1.7 cups).\n",
            "\n",
            "The easiest method to use the ice cream you have in your freezer is:\n",
            "\n",
            "1) Bring 4 ounces (150 milliliters or 2.8 cups) of ice cream to a boil and cover.\n",
            "\n",
            "2) Reduce heat to medium and let ice cream cool to room temperature until it reaches a soft consistency that you like.\n",
            "\n",
            "3) Remove ice cream from heat and place it in a bowl, or in a freezer bag.\n",
            "\n",
            "4) Pour ice cream into a container or container that you will keep the ice cream in.\n",
            "\n",
            "5) Store the ice cream in its container in its original place until you wish to make it the next time you want to enjoy a delicious ice cream.\n",
            "\n",
            "How to use it\n",
            "\n",
            "When to use it\n",
            "\n",
            "When the ice cream is at its best, it should have a smooth and rich texture. This is because it should be thick enough to coat your teeth and be soft enough to be absorbed easily.\n",
            "\n",
            "When the ice cream is at its worst, the ice cream might have a rough texture and be more or less chalky than you want.\n",
            "\n",
            "This happens because the ice cream is not as thick as you would expect it to be.\n",
            "\n",
            "This can happen when you are too impatient to wait for the ice cream to melt, or because the ice cream is too soft and is just about ready to be\n",
            "======================================== SAMPLE 3 ========================================\n",
            "We have all seen that a dog will eat anything that's in its immediate line of sight. Dogs will eat anything and everything!\n",
            "\n",
            "In fact, dogs can eat a human body in less than a second.\n",
            "\n",
            "The speed and accuracy that a dog can detect a person is called \"sighting speed\". If a dog could see someone's face with its nose, it would be able to tell the person was a dog and not a human.\n",
            "\n",
            "A human with a dog's eye-sight could only see about 50-75% of a full-size person's body. A dog's \"sighting\" speed would be 100%.\n",
            "\n",
            "A dog that can see you in your bedroom with its sight will probably be able to spot you on a street and walk away before you even realize it.\n",
            "\n",
            "A dog would not have to see anything at all to know whether or not a person in its line of sight was a human or not.\n",
            "\n",
            "This is a dog's brain and not ours!\n",
            "\n",
            "Dogs and humans share a brain\n",
            "\n",
            "Dogs do not have a \"brain\" in comparison! A dog does not have a brain to process the information it receives, and thus it cannot understand how humans think. The brain and the body are not the same thing!\n",
            "\n",
            "Dogs can only see so far into the darkness of a dark room\n",
            "\n",
            "The dog's visual system is not capable of processing a full-size human's visual field. The visual systems of humans are much larger and much more complex than the dog's visual system.\n",
            "\n",
            "The human brain's visual systems can be as much as a thousand times larger than the dog's visual system. A dog's visual systems will not be able to detect the light that you see from the street lights.\n",
            "\n",
            "Dogs can't see you in your bed\n",
            "\n",
            "A dog can still hear you when you're sleeping. A dog can still see you in the darkness of your bed, or if you are sitting on the couch. Dogs can still see you if you are in a dark place that they have never been before, like the dark bedroom you are in.\n",
            "\n",
            "Dogs are more social animals\n",
            "\n",
            "A dog that can smell a person in a dark room, can recognize you in the darkness, is social to a degree that humans are not! Dogs can recognize and trust people, but humans can not.\n",
            "\n",
            "A person can recognize another person in a dark room, but a dog can't see you in the dark!\n",
            "\n",
            "Dogs can hear what you do without your knowing it. The sound you make and the sounds your mouth makes are all part of your brain and are all processed by your brain. If you can hear that you are doing something with your mouth, then your brain has processed all of your sounds in order to process what you are saying! A person can hear what you are saying in a dark room, but a dog can't! Dogs are not smart enough to process what is said, so if you say something with your mouth, your brain can not process that information.\n",
            "\n",
            "You can't be a dog\n",
            "\n",
            "A person can recognize you without knowing that you are a dog.\n",
            "\n",
            "A dog can't even recognize the shape of a person's body! A dog can't recognize the shape of another person's body!\n",
            "\n",
            "A dog can't see you from a distance that a human can.\n",
            "\n",
            "A dog will never be able to spot another dog on a leash!\n",
            "\n",
            "A dog can't detect the color of your eyes, skin color, hair, or any other physical characteristic.\n",
            "\n",
            "A person can not tell the difference whether or not a dog has a \"dog's brain.\"\n",
            "\n",
            "A person who is a dog can see the color of a person's eyes, skin, hair, hair color, skin color, color of hair, or color of hair! A dog can not.<|endoftext|>\"We don't like this, but we're not going to change our approach.\"\n",
            "\n",
            "That's just one thing former U.S. Attorney General Michael Mukasey told a gathering of his former colleagues on Monday. Mukash said his job as special counsel investigating Russian involvement in the 2016 election was to ensure that Congress had the facts on their side.\n",
            "\n",
            "\"We don't like this, but we're not going to change our approach,\" he said. \"We have a very clear understanding and commitment to follow the facts to the letter.\"\n",
            "\n",
            "The comments, which came during a wide-ranging interview at a dinner sponsored by the New York Times, come as the White House continues to grapple with the fallout from last month's testimony of Trump campaign chairman Paul Manafort in which he denied making payments to pro-Russian political parties. Manafort has pleaded guilty in the case to one count of money laundering and one count of failing to report foreign bank accounts to the U.S. government.<|endoftext|>As a student at the University of Illinois at Urbana-Champaign, I have seen and written about how\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOO7SrNKfQYc",
        "colab_type": "text"
      },
      "source": [
        "# Conditional sample generation\n",
        "Generates text sampels conditional on some user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTcYA8Xr_S2E",
        "colab_type": "text"
      },
      "source": [
        "## Notes on flags:\n",
        "The code comes with a few flags available, with a default value:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B97LWti6Dep1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* **seed** = None || a random value is generated unless specified. give a specific integer value if you want to reproduce same results in the future.\n",
        "* **nsamples** = 1 || specify the number of samples you want to print\n",
        "* **length** = None || number of tokens (words) to print on each sample.\n",
        "* **batch_size** = 1 || how many inputs you want to process simultaneously. doesn't seem to affect the results.\n",
        "* **temperature** = 1 || scales logits before sampling prior to softmax.\n",
        "* **top_k** = 0 || truncates the set of logits considered to those with the highest values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNIO4f0iDk9O",
        "colab_type": "text"
      },
      "source": [
        "## Different usecases:\n",
        "\n",
        "\n",
        "**1.   Text completion**\n",
        "\n",
        "**2.  Question answering**\n",
        "\n",
        "**3.   Summarisation**\n",
        "\n",
        "**4.  Translation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfskdff44QlD",
        "colab_type": "text"
      },
      "source": [
        "### 1. Text Completion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFB86teG__A-",
        "colab_type": "text"
      },
      "source": [
        "Feed in some random text and see what the AI generates from that!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9oU0a5LJ0K3",
        "colab_type": "text"
      },
      "source": [
        "#### Example usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v-k0G1zJ27W",
        "colab_type": "text"
      },
      "source": [
        "> `!python3 src/interactive_conditional_samples.py --nsamples=2 --top_k=40 --temperature=.80 --model_name='345M'`\n",
        "\n",
        "\n",
        "> Model prompt >>> \"Our solar system consists of the inner and outer planets, separated by an asteroid belt. It has \"\n",
        "\n",
        "> Model prompt >>> \"The 10 best foods are: 1. Peanut butter jelly sandwiches 2. Marshmallows 3. Broccoli 4.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqv_iC-sJx68",
        "colab_type": "text"
      },
      "source": [
        "#### Try it out yourself!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QIdaQn5WkSf",
        "colab_type": "code",
        "outputId": "7759192f-b8b7-4f50-de4f-2acb9d4eb473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2706
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --nsamples=2 --top_k=40 --temperature=.80 --model_name='345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-10 05:20:16.855471: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-10 05:20:16.855745: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2f76680 executing computations on platform Host. Devices:\n",
            "2019-05-10 05:20:16.855778: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 05:20:17.002035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 05:20:17.002571: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2f75fa0 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 05:20:17.002601: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 05:20:17.002988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.39GiB\n",
            "2019-05-10 05:20:17.003016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 05:20:17.530797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 05:20:17.530881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 05:20:17.530896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 05:20:17.531154: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-10 05:20:17.531211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13914 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> Our solar system consists of the inner and outer planets, separated by an asteroid belt. It has\n",
            "2019-05-10 05:21:14.634269: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            " been estimated that more than half of the planets in our solar system are rocky planets.\n",
            "\n",
            "The main body of a solar system is the sun, which is around 3.8 million km in diameter. It is surrounded by a ring of cool, dense gas and dust. The ring is also composed of gas and dust. The outermost ring of our solar system is the Solar System, and we call it the Solar System.\n",
            "\n",
            "Our solar system consists of a number of planets in the outer Solar System and two major bodies, Jupiter and Saturn. The outermost planet, Jupiter, orbits the Sun at an average distance of 3.3 million km from Earth, and the innermost planet, Saturn, orbits at an average distance of 8.9 million km from the Sun. Jupiter also has a large moon in its orbit, called Enceladus.\n",
            "\n",
            "Most of the planets in our Solar System are rocky. As with our own solar system, the innermost planet is the rocky planet Uranus. About 90% of the planets in our Solar System are rocky planets. We call the outermost planet of our solar system, Neptune, \"the second most massive planet.\" Uranus has about 3 times the mass of Earth; Neptune has 5.4 times that mass. Earth has a mass of about 1,500 metric tons and Neptune has about 4,000 metric tons. Uranus is very cold and has a temperature of about minus 180 degrees Fahrenheit. The outermost planet, Pluto, lies about 3.6 million kilometers from our planet in the direction of the constellation of Aquarius. Pluto is also a dwarf planet.\n",
            "\n",
            "The innermost planet in our solar system is Jupiter, which is about 23 million km in diameter. It is surrounded by a ring of gas and dust, called the Solar System. The outermost planet, Saturn, orbits at an average distance of 17.8 million km from the Sun. Jupiter also has a small moon in its orbit, called Europa.\n",
            "\n",
            "A planet that is too close to our own sun, and too far away from its parent star, can suffer large amounts of heat and radiation. If a planet or a comet passes within about 5 million miles (8.7 million kilometers) of its parent star, it can be destroyed. A comet can also be destroyed by the heat of fusion, or the fusion of hydrogen to helium. This occurs when a comet passes in front of and between stars, creating a shock wave that can destroy most comets.\n",
            "\n",
            "Our solar system has\n",
            "======================================== SAMPLE 2 ========================================\n",
            " been estimated that the planet is made up of between 3 trillion and 8 trillion rocky bodies.\n",
            "\n",
            "The moon is similar in size to Earth in mass, but is closer to the sun with a surface temperature that is much cooler than Earth's. The moon's atmosphere and oceans are made of liquid water, which makes it vulnerable to ultraviolet radiation that would harm life on Earth – especially life on Earth's surface.\n",
            "\n",
            "With a surface temperature of around -250C, the moon has an average atmosphere temperature of 5.75 degrees centigrade (3.8 degrees Fahrenheit), making it habitable for life.\n",
            "\n",
            "Earth's atmosphere would be roughly -50 degrees centigrade (5 degrees Fahrenheit), making it also habitable for life.\n",
            "\n",
            "While the moon's surface temperatures are below the earth's surface, it is only about 20 per cent of the earth's surface.\n",
            "\n",
            "However, the outer planets, such as Pluto, do have their own unique atmospheric conditions.\n",
            "\n",
            "The outer planets\n",
            "\n",
            "The outer planets, such as Pluto and the gas giant Ganymede, have similar atmospheres, but are extremely cold and have a surface temperature of -380C.\n",
            "\n",
            "Some of Pluto's icy moons are found only on the largest of the outer planets, such as Charon, although more are found around the smaller moons of Enceladus.\n",
            "\n",
            "One such moon, Enceladus, is known to be home to two hydrothermal vents, but the vent itself could be only about 500 metres across, making it extremely large.\n",
            "\n",
            "Unlike the moon, Enceladus is not the closest object in the outer solar system. The moon orbits at just over 10 km in diameter, and orbits its parent body, Vesta, at a distance of about 1.5 times the size of Vesta.\n",
            "\n",
            "Enceladus has a very unusual climate profile.\n",
            "\n",
            "The moon has liquid water at its surface and is likely home to life from a variety of life forms.\n",
            "\n",
            "Enceladus is so cold that a heat pump would have to exist to keep the water warm. The largest such pump is believed to be the Kuiper belt, which is a region surrounding the moon.\n",
            "\n",
            "Hades\n",
            "\n",
            "Hades is a massive red dwarf star that is about 15 times the mass of the sun and is located in the constellation of Cancer.\n",
            "\n",
            "Hades is one of five such stars with supermassive black holes (G10) in the constellation of Boötes\n",
            "================================================================================\n",
            "Model prompt >>> The 10 best foods are: 1. Peanut butter jelly sandwiches 2. Marshmallows 3. Broccoli 4.\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Oreos 5. Apple Pie 6. Apple Ice Cream 7. Pretzels 8. Snickers 9. Apple slices 10. Choc chips\n",
            "\n",
            "\n",
            "3. Oreos 1. Oreos 2. Oreos 3. Oreos 4. Oreos 5. Oreos 6. Oreos 7. Oreos 8. Oreos 9. Oreos 10. Chips 11. Marshmallows 12. Apple 12. Ice Cream 13. Apple Ice Cream 14. Snickers 15. Fruity Pebbles 16. Cheeseburgers 17. Cheeseburgers 18. Oreos 19. Pretzels 20. Choc chips\n",
            "\n",
            "4. Cheeseburgers 1. Choc chips 2. Cheeseburgers 3. Cheeseburgers 4. Cheeseburgers 5. Choc Chips 6. Oreos 7. Oreos 8. Oreos 9. Oreos 10. Chips 11. Ice Cream 12. Cheeseburgers 13. Cheeseburgers 14. Cheeseburgers 15. Oreos 16. Cheeseburgers 17. Cheeseburgers 18. Cheeseburgers 19. Cheeseburgers 20. Cheeseburgers 21. Cheeseburgers 22. Cheeseburgers 23. Cheeseburgers 24. Cheeseburgers 25. Cheeseburgers 26. Cheeseburgers 27. Cheeseburgers 28. Cheeseburgers 29. Cheeseburgers 30. Cheeseburgers 31. Cheeseburgers 32. Cheeseburgers 33. Cheeseburgers 34. Cheeseburgers 35. Cheeseburgers 36. Cheeseburgers 37. Cheeseburgers 38. Cheeseburgers 39. Cheeseburgers 40. Cheeseburgers\n",
            "\n",
            "5. Ice Cream 1. Ice Cream 2. Ice Cream 3. Ice Cream 4. Ice Cream 5. Ice Cream 6. Ice Cream 7. Ice Cream 8. Ice Cream 9. Ice Cream 10. Cheeseburgers 11. Ice Cream 12. Cheeze 13. Ice Cream 14. Cheeze 15. Ice Cream 16. Ice Cream 17. Ice Cream 18. Oreos 19. Ice Cream 20. Ice Cream 21. Ice Cream 22. Ice Cream 23. Ice Cream 24. Ice Cream 25. Peanut Butter Jelly Sandwich 26. Peanut Butter\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Carrots 5. Mashed Potatoes 6. Broccoli Crisps 7. Cheezeburgers 8. Cheese and Beer 9. Chicken Nuggets 10. Green Beans, Rice, and Rice Flakes\n",
            "\n",
            "2. Eggs\n",
            "\n",
            "3. Eggs made out of milk and butter (or eggs) are a good choice for a low carb diet. Eggs are healthy for you. They are also low in fat, cholesterol and calories. Eggs are also a great source of Omega 3's. 4. Cheese\n",
            "\n",
            "Chill some cheeses before adding to your diet and they will taste better. Try the fresh mozzarella cheese available from the grocery store on occasion, or go to the cheese bar at any deli on a regular basis. These are not cheap and you want to be able to afford it.\n",
            "\n",
            "4. Chicken\n",
            "\n",
            "Bread or crackers can be added to your diet to help with weight loss. It is also a great source of protein. Chicken is high in protein and Omega 3's. 5. Cheese\n",
            "\n",
            "If you can't afford cheese, try to find a store that has cheese on the menu. Use it with your favorite crackers or crackers for breakfast or lunch! 6. Chicken Salad\n",
            "\n",
            "A salad with chicken, tomatoes, lettuce, and pickles will add a nice little boost to your low carb diet. 7. Broccoli\n",
            "\n",
            "Broccoli is also a great source of Omega 3's! 8. Spinach\n",
            "\n",
            "A vegetable rich in Vitamin D\n",
            "\n",
            "9. Corn\n",
            "\n",
            "Corn is a great low carb meal, especially when cooked low in sugar. 10. Applesauce\n",
            "\n",
            "Add applesauce to a low carb meal, and add more if you are feeling extra sweet. 11. Sausage\n",
            "\n",
            "Sausage is another great low carb meal, especially when cooked low in sugar. 12. Cheese\n",
            "\n",
            "It is also a good source of Omega 3's. 13. Roasted Garlic\n",
            "\n",
            "Roasted garlic, added to pasta, makes a great low carb meal. 14. Cheese Crackers\n",
            "\n",
            "It is also a good source of Omega 3's. 15. Egg\n",
            "\n",
            "Fruit, low carb or not, should be a part of any low carb meal. 16. Broccoli Crisps\n",
            "\n",
            "High in Omega 3's, broccoli is an easy low carb snack. 17. Green Beans\n",
            "\n",
            "Fruit, low carb or not, should be a part of any low carb meal. 18. Broccoli Flaked\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4dnu9FTwLNw",
        "colab_type": "text"
      },
      "source": [
        "### 2. Question-Answering\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldY01wWnAgDd",
        "colab_type": "text"
      },
      "source": [
        "Feed in a passage, and then some question/answer pairs (Q: blah blah? A: Blah blah.), and token `A:`. The AI will answer the previous ''`Q:`''\n",
        "\n",
        "\n",
        "Note, for a single word answer (i.e.: Yes/No, city), set flag `length=1`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz7jQrOZJjSU",
        "colab_type": "text"
      },
      "source": [
        "#### Example usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bbc7cFNJrIO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> `!python3 src/interactive_conditional_samples.py --nsamples=10 --top_k=40 --temperature=.80 --length=1 --model_name='345M'`\n",
        "\n",
        "> Model Prompt >>> \n",
        " \n",
        "> ```\n",
        "The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer\n",
        "Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in\n",
        "Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried\n",
        "the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started\n",
        "ahead of the 1936 Summer Olympics.\n",
        "After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was\n",
        "following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing\n",
        "ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of\n",
        "Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the\n",
        "event.\n",
        "Q: What was the length of the race?\n",
        "A: 137,000 km\n",
        "Q: Was it larger than previous ones?\n",
        "A: No\n",
        "Q: Where did the race begin?\n",
        "A: Olympia, Greece\n",
        "Q: Where did they go after?\n",
        "A: Athens\n",
        "Q: How many days was the race?\n",
        "A: seven\n",
        "Q: Did they visit any notable landmarks?\n",
        "A: Panathinaiko Stadium\n",
        "Q: And did they climb any mountains?\n",
        "A:\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4UQUQL-Jtwl",
        "colab_type": "text"
      },
      "source": [
        "#### Try it out yourself!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJm1OuwrLd2t",
        "colab_type": "code",
        "outputId": "6dda8a44-47f5-40a6-89e7-2cb54d2aff8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1363
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --nsamples=10 --top_k=40 --temperature=.80 --length=1 --model_name='345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-10 05:35:36.852390: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-10 05:35:36.852673: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1ba2680 executing computations on platform Host. Devices:\n",
            "2019-05-10 05:35:36.852730: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 05:35:36.998389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 05:35:36.998933: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1ba1fa0 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 05:35:36.998966: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 05:35:36.999334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.39GiB\n",
            "2019-05-10 05:35:36.999359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 05:35:37.525012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 05:35:37.525087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 05:35:37.525103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 05:35:37.525419: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-10 05:35:37.525477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13914 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics. After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event. Q: What was the length of the race? A: 137,000 km Q: Was it larger than previous ones? A: No Q: Where did the race begin? A: Olympia, Greece Q: Where did they go after? A: Athens Q: How many days was the race? A: seven Q: Did they visit any notable landmarks? A: Panathinaiko Stadium Q: And did they climb any mountains? A:\n",
            "2019-05-10 05:35:55.447454: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            " No\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Everest\n",
            "======================================== SAMPLE 3 ========================================\n",
            " Three\n",
            "======================================== SAMPLE 4 ========================================\n",
            " Everest\n",
            "======================================== SAMPLE 5 ========================================\n",
            " One\n",
            "======================================== SAMPLE 6 ========================================\n",
            " Yes\n",
            "======================================== SAMPLE 7 ========================================\n",
            " Mount\n",
            "======================================== SAMPLE 8 ========================================\n",
            " Yes\n",
            "======================================== SAMPLE 9 ========================================\n",
            " Mount\n",
            "======================================== SAMPLE 10 ========================================\n",
            " Mount\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnHDjbSszCOR",
        "colab_type": "text"
      },
      "source": [
        "### 3. Summarization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRyFm0nbAjiV",
        "colab_type": "text"
      },
      "source": [
        "Feed in a passage, and add and text *`TL;DR:`* or *`Summary:`* at the end, and the AI will try to summarise the text.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn5DaKOiKdK9",
        "colab_type": "text"
      },
      "source": [
        "#### Example usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oJCbdPdKfCx",
        "colab_type": "text"
      },
      "source": [
        "Note the following passage was obtained from a blog post about the [mars-water paradox](http://www.planetary.org/blogs/guest-blogs/2019/mars-water-stable-paradox.html).\n",
        "\n",
        "\n",
        "> `!python3 src/interactive_conditional_samples.py --nsamples=3 --length=100 --temperature=1 --model_name='345M'`\n",
        "\n",
        "> Model Prompt >>>\n",
        "\n",
        ">```\n",
        "Mars has been the most extensively studied planet in the Solar System, except of course Earth. For the last 25 years, these missions have focused on the search for life by “following the water.” Although we have acquired compelling evidence of flowing liquid water on early Mars, the fundamental question about how water could be stable under Martian atmospheric conditions remains unsolved. Everything we have learned about Mars points towards a freezing cold Martian climate that would be incapable of stabilizing liquid water throughout Mars’ history.\n",
        "The two ideas that suggest liquid water could not be stable on early Mars are the “Faint Young Sun Paradox” and the Martian orbit. The following is a summary of two recent papers about the problem of Mars’ early climate: “The climate of early Mars,” by Robin Wordsworth, and a book chapter by Robert Haberle and coauthors, “The Early Mars Climate System.” Mars today as we know it is a cold and dry desert with a thin atmosphere not capable of stabilizing liquid water on its surface. However, there is ample evidence that Mars had flowing liquid water on its surface about 4 to 3.7 billion years ago (named as the Noachian Period). The evidence gathered by Mars orbiters, rovers, and landers is geomorphological; (valley networks, crater lakes, purported Northern ocean, glacial landforms, etc.); mineralogical (iron- and magnesium-rich clay minerals, sulfates, chlorides, iron oxides, and oxyhydroxides, etc.); and isotopic (noble gases, nitrogen, hydrogen, oxygen and carbon).\n",
        "TL;DR: \n",
        "```\n",
        "\n",
        "> Model Prompt >>>\n",
        ">```\n",
        "Theodore McCarrick is the most senior Catholic figure to be dismissed from the priesthood in modern times.\n",
        "US Church officials said allegations he had sexually assaulted a teenager five decades ago were credible.\n",
        "Mr McCarrick, 88, had previously resigned but said he had \"no recollection\" of the alleged abuse.\n",
        "\"No bishop, no matter how influential, is above the law of the Church,\" Cardinal Daniel DiNardo, president of the United States Conference of Catholic Bishops said in a statement.\n",
        "\"For all those McCarrick abused, I pray this judgment will be one small step, among many, toward healing.\"\n",
        "The alleged abuses may have taken place too long ago for criminal charges to be filed because of the statute of limitations.\n",
        "Mr McCarrick was the archbishop of Washington DC from 2001 to 2006. Since his resignation last year from the College of Cardinals, he has been living in seclusion in a monastery in Kansas.\n",
        "He was the first person to resign as a cardinal since 1927.\n",
        "He is among hundreds of members of the clergy accused of sexually abusing children over several decades and his dismissal comes days before the Vatican hosts a summit on preventing child abuse.\n",
        "The Vatican said Pope Francis had ruled Mr McCarrick's expulsion from the clergy as definitive, and would not allow any further appeals against the decision. \n",
        "TL;DR: \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpw6EKUtKgzP",
        "colab_type": "text"
      },
      "source": [
        "#### Try it out yourself!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMes5yRQuXs4",
        "colab_type": "code",
        "outputId": "e4387e8c-ba12-43c4-a799-134c40301d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --nsamples=3 --length=100 --temperature=1 --model_name='345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-10 05:42:12.717897: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-10 05:42:12.718185: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x17ec680 executing computations on platform Host. Devices:\n",
            "2019-05-10 05:42:12.718223: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 05:42:12.865448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 05:42:12.866025: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x17ebfa0 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 05:42:12.866059: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 05:42:12.866427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.39GiB\n",
            "2019-05-10 05:42:12.866453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 05:42:13.400792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 05:42:13.400863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 05:42:13.400876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 05:42:13.401183: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-10 05:42:13.401233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13914 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> Mars has been the most extensively studied planet in the Solar System, except of course Earth. For the last 25 years, these missions have focused on the search for life by “following the water.” Although we have acquired compelling evidence of flowing liquid water on early Mars, the fundamental question about how water could be stable under Martian atmospheric conditions remains unsolved. Everything we have learned about Mars points towards a freezing cold Martian climate that would be incapable of stabilizing liquid water throughout Mars’ history. The two ideas that suggest liquid water could not be stable on early Mars are the “Faint Young Sun Paradox” and the Martian orbit. The following is a summary of two recent papers about the problem of Mars’ early climate: “The climate of early Mars,” by Robin Wordsworth, and a book chapter by Robert Haberle and coauthors, “The Early Mars Climate System.” Mars today as we know it is a cold and dry desert with a thin atmosphere not capable of stabilizing liquid water on its surface. However, there is ample evidence that Mars had flowing liquid water on its surface about 4 to 3.7 billion years ago (named as the Noachian Period). The evidence gathered by Mars orbiters, rovers, and landers is geomorphological; (valley networks, crater lakes, purported Northern ocean, glacial landforms, etc.); mineralogical (iron- and magnesium-rich clay minerals, sulfates, chlorides, iron oxides, and oxyhydroxides, etc.); and isotopic (noble gases, nitrogen, hydrogen, oxygen and carbon). TL;DR: \n",
            "2019-05-10 05:42:42.094420: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            " The climate of early Mars with flowing rock is  CMB 30B-22P\n",
            "  Fun facts about the study of the early Mars temperature: 1. Mars is a frigid planet  - the bottom of the atmosphere is 1-3°C below freezing with relative humidity 50% of Earth surface heat tolerance. 2. Almost all of Earth's orbital Mars stations us at least 0 G from Earth. 3. With the exception of Mars at its core,  the following space\n",
            "======================================== SAMPLE 2 ========================================\n",
            "???? TL;DR3 TL;DR4 TL;DR5 TL;DR6\n",
            "\n",
            "Clay brought to Mars Just how did land animals such as reindeer, giant tortoises, ducklings, penises, etc., land on Mars? We don't have a good answer to this since the sample possible on Mars from arachnids on Earth is insignificant compared to the 90&000 species beings that you might find on a Mars base. This strange equatorial rarely stream is\n",
            "======================================== SAMPLE 3 ========================================\n",
            "_____ Faint Young Sun Error on early Hydrogen Ocean Generate samples, calendar -- no problem, if your time is on-line [NJP] _____ The Michael Doubt Principle: Bad Mars Meteorology [PDF, 3MB] On page 203 of Michael Doubt's now bestselling book written with Mariana Sagan-Jones  the question of whether there should be atmosphere on Mars? Doubt dismisses the idea and states but,  \"There must have been water at\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdw9P3QdzA-e",
        "colab_type": "text"
      },
      "source": [
        "### 4. Translation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y8FkqnXAmqg",
        "colab_type": "text"
      },
      "source": [
        "Provide a few example translations, using the format of *`english_text = other_language_text`*, and then *`english_text =`*  at the end. The AI will try to translate the english text into the other language.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deI6hvnVK8EY",
        "colab_type": "text"
      },
      "source": [
        "#### Example usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYbnMqk5K-BO",
        "colab_type": "text"
      },
      "source": [
        "> `!python3 src/interactive_conditional_samples.py --nsamples=3 --temperature=1 --model_name='345M' `\n",
        "\n",
        "> Model Prompt >>>\n",
        "\n",
        "> ```\n",
        "Good morning. = Buenos días.\n",
        "I am lost. Where is the restroom? = Estoy perdido. ¿Dónde está el baño?\n",
        "How much does it cost? = ¿Cuánto cuesta?\n",
        "How do you say maybe in Spanish? = ¿Cómo se dice maybe en Español?\n",
        "Would you speak slower, please. = Por favor, habla mas despacio.\n",
        "Where is the book store? = ¿Dónde está la librería?\n",
        "At last a feminist comedian who makes jokes about men. = Por fin un cómico feminista que hace chistes sobre hombres.\n",
        "How old are you? = \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWcB6I4AK_os",
        "colab_type": "text"
      },
      "source": [
        "#### Try it out yourself!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3pueC6NEm5t",
        "colab_type": "code",
        "outputId": "0ff6d221-4f5e-4fe7-e059-d054c0ff5dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1278
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --nsamples=3 --temperature=1 --model_name='345M' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-10 05:43:46.722814: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-10 05:43:46.723095: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x272a680 executing computations on platform Host. Devices:\n",
            "2019-05-10 05:43:46.723130: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 05:43:46.871727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 05:43:46.872278: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2729fa0 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 05:43:46.872309: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 05:43:46.872736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.39GiB\n",
            "2019-05-10 05:43:46.872767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 05:43:47.423770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 05:43:47.423852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 05:43:47.423867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 05:43:47.424168: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-10 05:43:47.424223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13914 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> Good morning. = Buenos días. I am lost. Where is the restroom? = Estoy perdido. ¿Dónde está el baño? How much does it cost? = ¿Cuánto cuesta? How do you say maybe in Spanish? = ¿Cómo se dice maybe en Español? Would you speak slower, please. = Por favor, habla mas despacio. Where is the book store? = ¿Dónde está la librería? At last a feminist comedian who makes jokes about men. = Por fin un cómico feminista que hace chistes sobre hombres. How old are you? = \n",
            "2019-05-10 05:44:12.509210: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            "ésán de su salas, la banda ya los símores. where will you go next? = Si el papel esquina en del mundo. where is Corona? = Experimental Nissan the car Hi Holl Spirit limited. = Hola Holl Spirit! Introduce yourself. = Sur la desarrollo del sua drugstore & de bajo. Where the fuck is the Dollar Tree? = Tomo el los dramatico known as the Dollar Tree. Have you seen Camelot? = Il tutte solamente del cajón de barrio presente. Where's the water? = , doingotera es area libertà. Where's the Thin White Duke? = , manga no bico nosotros ma tanto sistema?\n",
            "\n",
            "RAW Paste Data\n",
            "\n",
            "SUGAR FLARES + SUGAR FLOOD = BASARA (?!\"Oh my back.\") Du bare party. = Mi fien al artículo, quiso ocho stationurado, está valor? What a mess than I imagined. = Cocó un affordarne hoja a mi casa (ustede morte a un bien). It is better to enter a forbidden book store or a book store that doesn't have any historic numer How do you compute? = Imperial al arigú nord ? How do you print there? = Copieró de cartas. You need money to print. = Copiería de cartas, pero llegar é son gloria. What are you doing, pushing out all your worthiest efforts that you may have forgotten to meet? = Sumas lilas más mortes de las mysticas Autres forgivened. Say Lay back, lay back. = ¿Cómo hidradoros antes? You would forgive a wooden football to be erased from the history books. = Pacífía alcun error de la historia. Contra la manifestoatlwych a tal humans el cazadero. \"What was the weed used to bake bread tree did not know about those who came after.\" Homage to Sam. = Pará el líbo. Madrid. Order War. Original version Friday, September 5, 2002 at 8:20 PM You are so nice. Tell me back or stay here. Just drop me a line. Absolutely! And to newer people carrying relationship gloves, the large maracas on the dog in the photo might supposedly mean \"\n",
            "======================================== SAMPLE 2 ========================================\n",
            "????? It is hard to trans role model. Come express yourself well in Spanish instead. = ¿Terá el sangre manera ya casa? Nice clothes and shoes, we should buy them. = ¿Lo mè? Would you like coffee? = ¿Pues parredo comida? Is he gay? = That would be offensive. = ????? We will be back soon. = En que sabe, sugún hombres tiempo son los trabajadores. Don't listen to him. = los trabajadores soy están está gente. It's too bad your family or friends would be jealous. = Los trabajadores están están es trabajo agradely da stor|Sae!|tro? Literally, the time will pass soon enough.....≡h� ft=Ch|US|coenderd|I think[23]Ó€ When it rained() accumulation\".[football emoji]😕 — Rules Of Data Science ✧ Campaign cheatV program💻 def getCampaignState( campaigns = None): id = campaigns.keys() campaignState = None try: #fill your info with data #No data value type=type where = campaigns.keys() equalAs = campaigns[id].completed Values = campaignState #fill a dictionary for you[type,-count] {'name', 'type', 'count')[''] = { \"name\", getCampaignState(ids[type],count) } #get campaign input winData = [] elif not campaigns and isinstance(campaigns, lists.OverlappingCategories): wins = transformAncestor(campaigns, sortOrdered(recipes['favorite'])) nextScore = transformAncestor(campaigns, sortOrdered(recipes['favorite'][','day'],sortCode(( %sum(good(pstr(stats['success'], 0), -gaussianblas) %grouping(float(random() * 10001,'charsets','utf8')))) %grouping(float(random() * 10001), 'utf8'))) %grouping(float(random() * 10001)]) %grouping(round((50 * 50), -gaussianblas), count), winData) except: wins.append((_(\"No records found in total\").format(winData)))] print itaqueMac\n",
            "======================================== SAMPLE 3 ========================================\n",
            "ípomós el lengua. What would you say if I gave you a little help? = ús tu te sabe me dará usted por possible. How much does it cost me? = Avendado costó si pero incluyaron. How much does it cost with light blue condoms? = Glorió el gringo de la ruga. I found the movie about breasts. = vadde su asíno por giranta les malos de cortados. Oh, do the cats pee in the library? = Revelar el gringo en el mocho, a machían. Is this really funny? = Vivido vor todo maximum que vengá la gascanita. [extra-extra-extra-IALC debate dyan:ua-dyan:ua] Vivido Vedá más instanto usted llegando translationos rejó lograr la opinión de thespian Documentation italiana especial:untu''ledgea (analysis 1922):ucelilyen á verbo a está beloro ya la digerera senza el tuempo verme el siguiente. (decONOUTE March 2002):una edicación sucilibuel insuperablee es perdiablo cómicos. Na que comprendo portatta la semana pehan una alimentación. demonizator 92\n",
            "\n",
            "\n",
            "The History Viewer 3. Introduction And well and good pruned boobs are plentiful in Portugal but do you know how Floridi grew up to be a modern Spain resident who is now in government with Leo Starra? The story below illustrates this very fascinating topic where we show how Spain changed from the golden child (Floridi), who started nursing to the slightly spoiled, but all star of the principality and soon became one of the leaders of the country.\n",
            "\n",
            "Gottal Residence by Juan de Venata As one of the Spanish princes serving on the 11th century squadron that first ceded Portugal, Nova Ledgetarrás came to Spain to be a supporter of the queen herself. Gran Carcias wanted and needed a protégé manor lord and needed Floridi's assistance. He died soon after in 708, leaving a stately house overlooking the sunbaked fortresses of León. Not long afterwards, Dunstan byname went to school at Plácido and\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}